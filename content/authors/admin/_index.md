---
# Display name
title: MacBERTh

# Is this the primary user of the site?
superuser: true

# Role/position/tagline
role: Language Models for Historical English and Dutch

# Organizations/Affiliations to show in About widget
organizations:
- name: PDI-SSH 2020
  url: https://pdi-ssh.nl/en/funded-projects/

# Short bio (displayed in user profile at end of posts)
# bio: My research interests include distributed robotics, mobile computing and programmable matter.

# Interests to show in About widget
# interests:
# - Artificial Intelligence
# - Computational Linguistics
# - Information Retrieval

# Education to show in About widget
# education:
#   courses:
#   - course: PhD in Artificial Intelligence
#     institution: Stanford University
#     year: 2012
#   - course: MEng in Artificial Intelligence
#     institution: Massachusetts Institute of Technology
#     year: 2009
#   - course: BSc in Artificial Intelligence
#     institution: Massachusetts Institute of Technology
#     year: 2008

# Social/Academic Networking
# For available icons, see: https://wowchemy.com/docs/getting-started/page-builder/#icons
#   For an email link, use "fas" icon pack, "envelope" icon, and a link in the
#   form "mailto:your-email@example.com" or "/#contact" for contact widget.
# social:
# - icon: envelope
#   icon_pack: fas
#   link: '/#contact'
# - icon: twitter
#   icon_pack: fab
#   link: https://twitter.com/GeorgeCushen
# - icon: graduation-cap  # Alternatively, use `google-scholar` icon from `ai` icon pack
#   icon_pack: fas
#   link: https://scholar.google.co.uk/citations?user=sIwtMXoAAAAJ
# - icon: github
#   icon_pack: fab
#   link: https://github.com/gcushen


# Link to a PDF of your resume/CV.
# To use: copy your resume to `static/uploads/resume.pdf`, enable `ai` icons in `params.toml`, 
# and uncomment the lines below.
# - icon: cv
#   icon_pack: ai
#   link: uploads/resume.pdf

# Enter email to display Gravatar (if Gravatar enabled in Config)
email: ""

# Highlight the author in author lists? (true/false)
highlight_name: true
---

MacBERTh is the cover term for a suite of language models (more specifically, BERT models) pre-trained on historical textual material (date range: 1450-1950).

Researchers who interpret and analyse historical textual material are well-aware that languages are subject to change over time, and that the way in which concepts and discourses of class, gender, norms and prestige function in different time periods. As such, it is quite important that the interpretation of textual/linguistic material from the past is not approached from a present-day point-of-view, which is why NLP models pre-trained on present-day language data are less than ideal candidates for the job. That's where MacBERTh can help.

At present, a model [pre-trained on historical English (1450-1950)](http://insertlinktoNLP4DH) has been published in the [huggingface repository](https://huggingface.co/emanjavacas/MacBERTh/tree/main). The release of a Dutch historical model is planned for 2022.

**How to cite:**

Manjavacas, Enrique & Lauren Fonteyn. MacBERTh: Development and Evaluation of a Historically Pre-trained
Language Model for English (1450-1950). 
