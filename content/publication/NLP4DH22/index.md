---
abstract: >-
  Recent approaches to Word Sense Disambiguation (WSD) have profited from the enhanced 
  contextualized word representations coming from contemporary Large Language Models (LLMs). 
  This advancement is accompanied by a renewed interest in WSD applications in Humanities 
  research, where the lack of suitable, specific WSD-annotated resources is a hurdle in 
  developing ad-hoc WSD systems. Because they can exploit sentential context, LLMs are 
  particularly suited for disambiguation tasks. Still, the application of LLMs is often limited
  to linear classifiers trained on top of the LLM architecture. In this paper, we follow recent
  developments in non-parametric learning and show how LLMs can be efficiently fine-tuned to 
  achieve strong few-shot performance on WSD for historical languages (English and Dutch, 
  date range: 1450-1950). We test our hypothesis using (i) a large, general evaluation set taken
  from large lexical databases, and (ii) a small real-world scenario involving an ad-hoc WSD
  task. Moreover, this paper marks the release of GysBERT, a LLM for historical Dutch.

url_pdf: ""
title: "MacBERTh: Development and Evaluation of a  Historically Pre-trained
  Language Model for English (1450-1950)"
publication_types:
  - "1"
authors:
  - Enrique Manjavacas
  - Lauren Fonteyn
summary: In this paper, we follow recent developments in non-parametric learning and show how LLMs can be efficiently fine-tuned to achieve strong few-shot performance on WSD for historical languages. We test our hypothesis using (i) a large, general evaluation set taken from large lexical databases, and (ii) a small real-world scenario involving an ad-hoc WSD task. Moreover, this paper marks the release of GysBERT, a LLM for historical Dutch (1500-1950).
url_dataset: ""
url_project: ""
publication_short: In *Natural Language Processing for Digital Humanities (NLP4DH)*
url_source: ""
url_video: ""
author_notes:
  - First autor
  - Second author
doi: ""
publication: In *Proceedings of the 2nd International Workshop on Natural Language Processing for Digital Humanities (NLP4DH)*
featured: true
tags: []
image:
  caption: "Image credit: MacBERTh team"
  focal_point: ""
  preview_only: true
date: "2022"
url_slides: ""
publishDate: 2022-11-4T00:00:00Z
url_poster: ""
url_code: ""
---

GysBERT is available as **emanjavacas/GysBERT** from the [transformers repository](https://huggingface.co/emanjavacas/GysBERT) (Wolf et al. 2019).
