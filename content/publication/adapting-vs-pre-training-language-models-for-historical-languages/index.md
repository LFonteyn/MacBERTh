---
title: "[Pre-Print] Adapting vs Pre-training Language Models for Historical Languages"
publication_types:
  - "3"
authors: []
abstract: >-
  As large language models such as BERT are becoming increasingly popular in
  Digital Humanities (DH), the question has arisen as to how such models can be
  made suitable for application to specific textual domains, including that of
  ‘historical text’. Large language models like BERT can be pretrained from
  scratch on a specific textual domain and achieve strong performance on a
  series of downstream tasks. However, this is a costly endeavour, both in terms
  of the computational resources as well as the substantial amounts of training
  data it requires. An appealing alternative, then, is to employ existing
  ‘general purpose’ models (pre-trained on present-day language) and
  subsequently adapt them to a specific domain by further pre-training. Focusing
  on the domain of historical text in English, this paper demonstrates that
  pre-training on domain-specific (i.e. historical) data from scratch yields a
  generally stronger background model than adapting a present-day language
  model.

  We show this on the basis of a variety of downstream tasks, ranging from established tasks such as Part-of-Speech tagging, Named Entity Recognition and Word Sense Disambiguation, to ad-hoc tasks like Sentence Periodization, which are specifically designed to test historically relevant processing.
draft: false
featured: true
image:
  filename: featured.png
  focal_point: Smart
  preview_only: true
  caption: "Image Credits: MacBERTh team"
summary: >-
  This is a pre-print. Please cite the published paper. Focusing on the domain
  of historical text in English, this paper demonstrates that pre-training on
  domain-specific (i.e. historical) data from scratch yields a generally
  stronger background model than adapting a present-day language model.

  We show this on the basis of a variety of downstream tasks, ranging from established tasks such as Part-of-Speech tagging, Named Entity Recognition and Word Sense Disambiguation, to ad-hoc tasks like Sentence Periodization, which are specifically designed to test historically relevant processing.
date: 2022-03-08T17:21:00.996Z
---
