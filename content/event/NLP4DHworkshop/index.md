---
abstract: The new pre-train-then-fine-tune paradigm in Natural Language
  Processing (NLP) has made important performance gains accessible to a wider
  audience. Once pre-trained, deploying a large language model presents
  comparatively small infrastructure requirements, and offers robust performance
  in many NLP tasks. The Digital Humanities (DH) community has been an early
  adapter of this paradigm. Yet, a large part of this community is concerned
  with the application of NLP algorithms to historical texts, for which large
  models pre-trained on contemporary text may not provide optimal results. In
  the present paper, MacBERTh, a transformer-based language model pre-trained on
  historical English. We exhaustively assess the benefits of this historically
  pre-trained language model on a large set of relevant downstream tasks. Our
  experiments highlight that, despite some differences across target time
  periods, pre-training on historical language from scratch outperforms models
  pre-trained on present-day language and later adapted to historical language.
url_pdf: ""
summary: This talk introduces MacBERTh, a transformer-based language model
  pre-trained on historical English. We exhaustively assess the benefits of this
  historically pre-trained language model on a large set of relevant downstream
  tasks.
title: Development and Evaluation of a  Historically Pre-trained Language Model
  for English (1450-1950)
location: Online
date: 2021-12-19T10:15:00.000Z
date_end: 2021-12-19T10:35:00.000Z
all_day: false
event: NLP4DH
event_url: https://rootroo.com/en/nlp4dh-workshop/
featured: false
authors:
  - Enrique Manjavacas & Lauren Fonteyn
url_video: ""
url_slides: ""
links: null
publishDate: 2021-12-09T09:13:37.311Z
tags: []
image:
  caption: "Image credit: MacBERTh team"
  focal_point: Right
url_code: ""
---

**Slides will be uploaded closer to date**